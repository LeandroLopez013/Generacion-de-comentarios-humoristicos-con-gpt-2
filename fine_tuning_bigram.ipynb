{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 15:22:34.002734: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-20 15:22:34.014453: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-20 15:22:34.017947: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-20 15:22:34.027187: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-20 15:22:34.976418: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import (Trainer, TrainingArguments, EarlyStoppingCallback,\n",
    "                          DataCollatorForLanguageModeling, AutoModelForCausalLM, AutoTokenizer)\n",
    "from datasets import load_dataset, Dataset\n",
    "from typing import Optional, Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import accelerate\n",
    "from huggingface_hub import notebook_login\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('mrm8488/CHISTES_spanish_jokes', split='train')\n",
    "dataset.set_format('pandas')\n",
    "df = dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>keywords</th>\n",
       "      <th>funny</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>- ¡Rápido, necesitamos sangre!\\n- Yo soy 0 pos...</td>\n",
       "      <td>sangre</td>\n",
       "      <td>1</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>- ¿Cuál es el mejor portero del mundial? \\n- E...</td>\n",
       "      <td>futbol,porteros</td>\n",
       "      <td>1</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>El otro día unas chicas llamarón a mi puerta y...</td>\n",
       "      <td>dinero,agua</td>\n",
       "      <td>1</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>- Andresito, ¿qué planeta va después de Marte?...</td>\n",
       "      <td>planetas</td>\n",
       "      <td>1</td>\n",
       "      <td>profesiones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>- ¿Por qué Bob Esponja no va al gimnasio? \\n- ...</td>\n",
       "      <td>esponja,gimnasios</td>\n",
       "      <td>1</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Van dos ciegos y le dice uno al otro: \\n- Ojal...</td>\n",
       "      <td>ciegos</td>\n",
       "      <td>1</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Noticia de última hora!! \\n\\nMuere una suegra ...</td>\n",
       "      <td>canarias,coches,noticias</td>\n",
       "      <td>2</td>\n",
       "      <td>familia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>– Mamá, mamá, en el colegio dicen que estoy lo...</td>\n",
       "      <td>locos,sillas</td>\n",
       "      <td>1</td>\n",
       "      <td>familia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>– Mamá, mamá, ¿me haces un bocata de jamón?\\n–...</td>\n",
       "      <td>madres,jamón</td>\n",
       "      <td>1</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>- Qué pasa si te expulsan de cuatro univerdade...</td>\n",
       "      <td>universitarios,universidades</td>\n",
       "      <td>1</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  \\\n",
       "0   0  - ¡Rápido, necesitamos sangre!\\n- Yo soy 0 pos...   \n",
       "1   1  - ¿Cuál es el mejor portero del mundial? \\n- E...   \n",
       "2   2  El otro día unas chicas llamarón a mi puerta y...   \n",
       "3   3  - Andresito, ¿qué planeta va después de Marte?...   \n",
       "4   4  - ¿Por qué Bob Esponja no va al gimnasio? \\n- ...   \n",
       "5   5  Van dos ciegos y le dice uno al otro: \\n- Ojal...   \n",
       "6   6  Noticia de última hora!! \\n\\nMuere una suegra ...   \n",
       "7   7  – Mamá, mamá, en el colegio dicen que estoy lo...   \n",
       "8   8  – Mamá, mamá, ¿me haces un bocata de jamón?\\n–...   \n",
       "9   9  - Qué pasa si te expulsan de cuatro univerdade...   \n",
       "\n",
       "                       keywords  funny     category  \n",
       "0                        sangre      1        otros  \n",
       "1               futbol,porteros      1        otros  \n",
       "2                   dinero,agua      1        otros  \n",
       "3                      planetas      1  profesiones  \n",
       "4             esponja,gimnasios      1        otros  \n",
       "5                        ciegos      1        otros  \n",
       "6      canarias,coches,noticias      2      familia  \n",
       "7                  locos,sillas      1      familia  \n",
       "8                  madres,jamón      1        otros  \n",
       "9  universitarios,universidades      1        otros  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- ¡Rápido, necesitamos sangre!\n",
      "- Yo soy 0 positivo.\n",
      "- Pues muy mal, necesitamos una mentalidad optimista. - ¡Rápido, ¡Rápido, necesitamos necesitamos sangre! sangre! - - Yo Yo soy soy 0 0 positivo. positivo. - - Pues Pues muy muy mal, mal, necesitamos necesitamos una una mentalidad mentalidad optimista.\n"
     ]
    }
   ],
   "source": [
    "def generate_ngrams(text: str, n: int) -> list:\n",
    "    words = text.split()\n",
    "    return [' '.join(words[i:i + n]) for i in range(len(words) - n + 1)]\n",
    "\n",
    "# Crear una nueva columna solo con bigramas\n",
    "df['bigrams'] = df['text'].apply(lambda x: generate_ngrams(x, 2))\n",
    "\n",
    "# Unimos los bigramas con el texto original para enriquecer los ejemplos\n",
    "df['text_enriched'] = df.apply(\n",
    "    lambda row: row['text'] + ' ' + ' '.join(row['bigrams']), axis=1\n",
    ")\n",
    "\n",
    "# Ver el primer ejemplo de text_enriched\n",
    "print(df['text_enriched'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# veo el type de df\n",
    "print(type(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando el conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kmansilla/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"DeepESP/gpt2-spanish-medium\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(max_len):\n",
    "    def _preprocess_function(examples):\n",
    "        return tokenizer(\n",
    "            examples['text_enriched'],\n",
    "            max_length=max_len,\n",
    "            truncation=True,\n",
    "            padding='longest',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "    return _preprocess_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 2419/2419 [00:00<00:00, 3403.99 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento: 1693 muestras\n",
      "Prueba: 726 muestras\n",
      "- Doctor, doctor, tiene que ayudarme! No se que me pasa que enseguida pierdo los nervios y me pongo a insultar a todo el mundo.\n",
      "- Está bien. Cuéntame sobre el asunto.\n",
      "- ¿Y qué cree que estoy haciendo, pedazo de imbécil? - Doctor, Doctor, doctor, doctor, tiene tiene que que ayudarme! ayudarme! No No se se que que me me pasa pasa que que enseguida enseguida pierdo pierdo los los nervios nervios y y me me pongo pongo a a insultar insultar a a todo todo el el mundo. mundo. - - Está Está bien. bien. Cuéntame Cuéntame sobre sobre el el asunto. asunto. - - ¿Y ¿Y qué qué cree cree que que estoy estoy haciendo, haciendo, pedazo pedazo de de imbécil?<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# Cargar y preparar el dataset enriquecido\n",
    "dataset = Dataset.from_pandas(df[['text_enriched']])\n",
    "dataset = dataset.remove_columns(\n",
    "    [col for col in dataset.column_names if col != 'text_enriched'])\n",
    "\n",
    "# Tokenización en paralelo\n",
    "tokenized_dataset = dataset.map(preprocess_function(\n",
    "    max_len=256), batched=True, num_proc=4)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(\n",
    "    [col for col in tokenized_dataset.column_names if col != 'input_ids'])\n",
    "\n",
    "# División del dataset\n",
    "tokenized_dataset = tokenized_dataset.train_test_split(train_size=0.70)\n",
    "\n",
    "# Asegurar formato PyTorch\n",
    "tokenized_dataset.set_format('torch')\n",
    "tokenized_dataset\n",
    "\n",
    "# Verificar las muestras tokenizadas\n",
    "print(f\"Entrenamiento: {len(tokenized_dataset['train'])} muestras\")\n",
    "print(f\"Prueba: {len(tokenized_dataset['test'])} muestras\")\n",
    "print(tokenizer.decode(tokenized_dataset['train'][0]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 8\n",
    "logging_steps = len(tokenized_dataset['train']) // batch_size\n",
    "\n",
    "# Definimos los parámetros de entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./hf-gpt',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=epochs,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=logging_steps,\n",
    "    report_to=\"tensorboard\",  # Reportar métricas a TensorBoard\n",
    "    save_total_limit=2,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False\n",
    ")\n",
    "\n",
    "# Crear el callback de early stopping\n",
    "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=3)\n",
    "\n",
    "# Definir el entrenador\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False),\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[early_stopping_callback]\n",
    ")\n",
    "\n",
    "# tensorboard --logdir=./logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descripcion de los argumentos:\n",
    "- `num_train_epochs`: Número de iteraciones de entrenamiento.\n",
    "- `learning_rate`: Tasa de aprendizaje para el optimizador, Un valor más alto puede acelerar el entrenamiento, pero un valor demasiado alto puede hacer que el modelo no converja.\n",
    "- `per_device_train_batch_size`: Tamaño del lote por dispositivo de entrenamiento.\n",
    "- `per_device_eval_batch_size`: Tamaño del lote por dispositivo de evaluación.\n",
    "- `weight_decay`: Tasa de decaimiento de los pesos, que ayuda a evitar el sobreajuste al añadir una penalización a los pesos grandes.\n",
    "- `eval_strategy`: significa que evalua al final de cada epoch.\n",
    "- `save_strategy`: significa que guarda el modelo al final de cada epoch.\n",
    "- `load_best_model_at_end`: significa que carga el mejor modelo al final del entrenamiento.\n",
    "- `logging_steps`: Cada cuántos pasos se imprime el log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1908' max='2120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1908/2120 4:07:22 < 27:30, 0.13 it/s, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.599300</td>\n",
       "      <td>1.966256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.801700</td>\n",
       "      <td>1.767062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.564200</td>\n",
       "      <td>1.710328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.406000</td>\n",
       "      <td>1.687153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.305300</td>\n",
       "      <td>1.676827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.219500</td>\n",
       "      <td>1.675656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.150600</td>\n",
       "      <td>1.677190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.113700</td>\n",
       "      <td>1.677086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.071300</td>\n",
       "      <td>1.678793</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1908, training_loss=1.4687158786525767, metrics={'train_runtime': 14852.6445, 'train_samples_per_second': 1.14, 'train_steps_per_second': 0.143, 'total_flos': 7075306241261568.0, 'train_loss': 1.4687158786525767, 'epoch': 9.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('trained-gpt2-bigram/tokenizer_config.json',\n",
       " 'trained-gpt2-bigram/special_tokens_map.json',\n",
       " 'trained-gpt2-bigram/vocab.json',\n",
       " 'trained-gpt2-bigram/merges.txt',\n",
       " 'trained-gpt2-bigram/added_tokens.json',\n",
       " 'trained-gpt2-bigram/tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Guardamos el modelo (solo correr si se vuelve a entrenar el modelo)\n",
    "# trainer.save_model('trained-gpt2-bigram')\n",
    "# tokenizer.save_pretrained('trained-gpt2-bigram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correr para cargar el modelo pre-entrenado\n",
    "model = AutoModelForCausalLM.from_pretrained('trained-gpt2-bigram')\n",
    "tokenizer = AutoTokenizer.from_pretrained('trained-gpt2-bigram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa233225df0a4a14a76c4f170958565a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#subo a huggingface\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c47575f8692f4f4d9c0b0ded5097ed7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo subido a Hugging Face\n"
     ]
    }
   ],
   "source": [
    "model.push_to_hub('kevmansilla/generate_jokes_bigram')\n",
    "tokenizer.push_to_hub('kevmansilla/generate_jokes_bigram')\n",
    "print('Modelo subido a Hugging Face')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kmansilla/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/kmansilla/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/kmansilla/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:615: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chiste generado: Un doctor le dice a su paciente:\n",
      "- Doctor, tengo un problema. Un médico me ha dicho que tiene usted una enfermedad venérea y no sé cómo se llama la enfermedad pero si lo sabe es porque el otro día vino al consultorio del cirujano para decirme que tenía problemas de corazón.\n"
     ]
    }
   ],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "class StopOnRepetition(StoppingCriteria):\n",
    "    def __init__(self, tokenizer, max_repeats=2):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_repeats = max_repeats\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        decoded_text = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "        words = decoded_text.split()\n",
    "        \n",
    "        # Detectar si hay demasiadas repeticiones consecutivas\n",
    "        if len(words) > 3 and len(set(words[-self.max_repeats:])) == 1:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "def generate_joke_from_prompt(prompt: str, max_length: int = 100, temperature: float = 0.7) -> str:\n",
    "    \"\"\"\n",
    "    Genera un chiste coherente a partir de un prompt usando el modelo entrenado.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): Inicio o tema del chiste.\n",
    "        max_length (int): Longitud máxima del texto generado.\n",
    "        temperature (float): Controla la creatividad del modelo.\n",
    "\n",
    "    Returns:\n",
    "        str: El chiste generado.\n",
    "    \"\"\"\n",
    "    # Cargar modelo y tokenizer\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained('trained-gpt2-bigram')\n",
    "    model = AutoModelForCausalLM.from_pretrained('trained-gpt2-bigram').to(device)\n",
    "\n",
    "    # Tokenizar el prompt\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    # Crear el criterio de parada\n",
    "    stopping_criteria = StoppingCriteriaList([StopOnRepetition(tokenizer)])\n",
    "\n",
    "    # Generar chiste con parámetros optimizados\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        top_k=50,  # Limitar las opciones para cada palabra\n",
    "        top_p=0.9,  # Nucleus sampling para mayor diversidad\n",
    "        repetition_penalty=1.5,\n",
    "        no_repeat_ngram_size=2,  # Evitar repeticiones de bigramas\n",
    "        stopping_criteria=stopping_criteria,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Decodificar el texto generado\n",
    "    joke = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    return joke\n",
    "\n",
    "# Ejemplo de uso\n",
    "prompt = \"Un doctor le dice a su paciente\"\n",
    "chiste = generate_joke_from_prompt(prompt, max_length=60)\n",
    "print(f\"Chiste generado: {chiste}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
