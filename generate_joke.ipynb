{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargo modelo de huggingface\n",
    "\n",
    "import torch\n",
    "from transformers import (AutoModelForCausalLM, StoppingCriteria, StoppingCriteriaList, AutoTokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo en español "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1109d7a4db7c4ebbb647874d9ddf0843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  24%|##4       | 346M/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92110fa4277f497fa049956eb9c978e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d3bc56ed3f1412fb5387343b06838d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.65k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f2f49acece34bbb9d3d38ff51f1e579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/840k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2acab8f6c50c43baa227d3f9219cb7ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/498k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9069fc41b3da4e55a5ecf89bba4c3cee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.19M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e93a4be0c674580a9484add9ded2b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/771 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cargo modelo with stop - base en español\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'kevmansilla/generate_jokes_with_stop')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'kevmansilla/generate_jokes_with_stop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_humor_comment(prompt: str, max_length: int = 150, num_return_sequences: int = 1) -> str:\n",
    "    '''\n",
    "    Genera un comentario humorístico basado en un prompt usando el modelo GPT-2 entrenado.\n",
    "    \n",
    "    Args:\n",
    "    - prompt (str): El texto o conjunto de palabras inicial para generar el comentario.\n",
    "    - max_length (int): La longitud máxima del comentario generado.\n",
    "    - num_return_sequences (int): El número de secuencias generadas. \n",
    "    \n",
    "    Returns:\n",
    "    - str: El comentario humorístico generado.\n",
    "    '''\n",
    "    model.eval()  # Poner el modelo en modo evaluación\n",
    "\n",
    "    # Tokenizamos el prompt con padding y truncation en la fase de tokenización\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True,\n",
    "                       truncation=True, max_length=max_length)\n",
    "\n",
    "    # Generamos texto con el modelo entrenado\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            no_repeat_ngram_size=2,  # Evitar repetición de n-gramas\n",
    "            top_k=50,  # Reducir el espacio de búsqueda\n",
    "            top_p=0.95,  # Aplicar nucleus sampling\n",
    "            temperature=0.7,  # Ajustar la creatividad de la generación\n",
    "            do_sample=True  # Permitir sampling en vez de greedy decoding\n",
    "        )\n",
    "\n",
    "    # Decodificar el texto generado de nuevo a formato legible\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profesor jaimito, te cuento una historia de tu vida.\n",
      "- ¿Cómo te llamas?\n",
      "-¿ Jaimita? ¿Y cómo te fue en la escuela? \n",
      "Jaimitos, Jaime, si te soy sincero yo no sabía que fueras español. Te llamaba Jaimado, porque el único español que se acuerda es el del penalty.....\n",
      "..... el penalti!...porque el que te habla es Jaimi!\n",
      "-¡Jaime!, me acuerdo de Jaibo!... ¿El que me habla?... ¡el que ríe!. ¡El del bigamia! ¡Jaimi Jaume!......el del chulo!.....¡el de la cara de leche\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de uso\n",
    "keywords = \"profesor jaimito\"\n",
    "comentario_generado = generate_humor_comment(keywords, max_length=150)\n",
    "print(comentario_generado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_humor_comment(keywords: list, max_length: int = 150, num_return_sequences: int = 1) -> str:\n",
    "    '''\n",
    "    Genera un comentario humorístico basado en una lista de keywords usando el modelo GPT-2 entrenado.\n",
    "    \n",
    "    Args:\n",
    "    - keywords (list): Lista de palabras clave para generar el comentario.\n",
    "    - max_length (int): La longitud máxima del comentario generado.\n",
    "    - num_return_sequences (int): El número de secuencias generadas. \n",
    "    \n",
    "    Returns:\n",
    "    - str: El comentario humorístico generado.\n",
    "    '''\n",
    "    model.eval()  # Poner el modelo en modo evaluación\n",
    "\n",
    "    # Unir las palabras clave en una cadena\n",
    "    prompt = ' '.join(keywords)\n",
    "\n",
    "    # Tokenizamos el prompt con padding y truncation en la fase de tokenización\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True,\n",
    "                       truncation=True, max_length=max_length)\n",
    "\n",
    "    # Generamos texto con el modelo entrenado\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            no_repeat_ngram_size=2,  # Evitar repetición de n-gramas\n",
    "            top_k=50,  # Reducir el espacio de búsqueda\n",
    "            top_p=0.95,  # Aplicar nucleus sampling\n",
    "            temperature=0.7,  # Ajustar la creatividad de la generación\n",
    "            do_sample=True  # Permitir sampling en vez de greedy decoding\n",
    "        )\n",
    "\n",
    "    # Decodificar el texto generado de nuevo a formato legible\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jaimito profesor: \n",
      "- ¿Sabes que te he dado 5 veces el mismo bofetón que tu padre? .... ..........\n",
      "-...\n",
      "-¡¡¡AAAAAHH!!!!!... ¿Cuánto es un bofetonazo?.  ¿Qué te ha dado?....\n",
      "-¿Cuánto?.. ¡¡HANAAHAAATH!!!... ¡AAASAAAMH!.!!.. ¡HASAMAAADORO! ;.. ¡¡AAARHAThAAGHOR!! ¡¡¡NOAAAPHYAA!!!!!. ¡¡HECHAMÁ!!... ¡¡NOHENHÁHE!!! ¡¡¿QU\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de uso\n",
    "keywords = ['jaimito', 'profesor']\n",
    "comentario_generado = generate_humor_comment(keywords, max_length=150)\n",
    "print(comentario_generado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo con base en ingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargo modelo con base en ingles\n",
    "model2 = AutoModelForCausalLM.from_pretrained(\n",
    "    'kevmansilla/generate_jokes_english')\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(\n",
    "    'kevmansilla/generate_jokes_english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo con bigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5df4db3fcd744d3aef3b516cbd7d88f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "923666a24c2444d3a52ee0f8e35acef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c81d704b151489a9f5ba669d64b6d50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d00b3df327d4d998672f10fdd541bfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/840k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97bc8bebf2d44be79f1b607d2ed02a36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/498k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1181ee675fcf43c2b77dfb3561e9d4f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.19M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0fa9a8a5c37482d938a440c47d016a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/771 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cargo modelo con bigrama\n",
    "model3 = AutoModelForCausalLM.from_pretrained(\n",
    "    'kevmansilla/generate_jokes_bigram')\n",
    "tokenizer3 = AutoTokenizer.from_pretrained(\n",
    "    'kevmansilla/generate_jokes_bigram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopOnRepetition(StoppingCriteria):\n",
    "    def __init__(self, tokenizer, max_repeats=2):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_repeats = max_repeats\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        decoded_text = self.tokenizer.decode(\n",
    "            input_ids[0], skip_special_tokens=True)\n",
    "        words = decoded_text.split()\n",
    "\n",
    "        # Detectar si hay demasiadas repeticiones consecutivas\n",
    "        if len(words) > 3 and len(set(words[-self.max_repeats:])) == 1:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "def generate_joke_from_prompt(prompt: str, max_length: int = 100, temperature: float = 0.7) -> str:\n",
    "    '''\n",
    "    Genera un chiste coherente a partir de un prompt usando el modelo entrenado.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): Inicio o tema del chiste.\n",
    "        max_length (int): Longitud máxima del texto generado.\n",
    "        temperature (float): Controla la creatividad del modelo.\n",
    "\n",
    "    Returns:\n",
    "        str: El chiste generado.\n",
    "    '''\n",
    "    # Tokenizar el prompt\n",
    "    input_ids = tokenizer3(prompt, return_tensors='pt').input_ids\n",
    "\n",
    "    # Crear el criterio de parada\n",
    "    stopping_criteria = StoppingCriteriaList([StopOnRepetition(tokenizer3)])\n",
    "\n",
    "    # Generar chiste con parámetros optimizados\n",
    "    output = model3.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        top_k=50,  # Limitar las opciones para cada palabra\n",
    "        top_p=0.9,  # Nucleus sampling para mayor diversidad\n",
    "        repetition_penalty=1.5,\n",
    "        no_repeat_ngram_size=2,  # Evitar repeticiones de bigramas\n",
    "        stopping_criteria=stopping_criteria,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=tokenizer3.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Decodificar el texto generado\n",
    "    joke = tokenizer3.decode(output[0], skip_special_tokens=True)\n",
    "    return joke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/.pyenv/versions/3.9.15/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/kevin/.pyenv/versions/3.9.15/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/kevin/.pyenv/versions/3.9.15/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:615: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chiste generado: Un doctor le dice a su paciente:\n",
      "- Doctor, tengo un problema. Un médico me ha dicho que tiene usted una enfermedad venérea y no sé cómo se llama la enfermedad pero si lo sabe es porque el otro día vino al consultorio del cirujano para decirme que tenía problemas de corazón.\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de uso\n",
    "prompt = 'Un doctor le dice a su paciente'\n",
    "chiste = generate_joke_from_prompt(prompt, max_length=60)\n",
    "print(f'Chiste generado: {chiste}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo question answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a2362ab9ca64715960f973c20976d15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb863f4feb514bba953274d2364e0289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c0de7e541fb429c8be65af2f09d8d47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1c16e4b4ec543ab8b15993008ba6e74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6398b050e5142ca96d5886b9bd72b35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/840k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bde222f93c7c40eeba9afaf4a5a094f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/498k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3be3d0fb1b4400f94ea012c7c603520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.19M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb13fe60f2b440ba8111d847a9607a02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/771 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cargo el modelo de hugging face\n",
    "model4 = AutoModelForCausalLM.from_pretrained(\n",
    "    'kevmansilla/generate_jokes_question_answer')\n",
    "tokenizer4 = AutoTokenizer.from_pretrained(\n",
    "    'kevmansilla/generate_jokes_question_answer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_short_joke(prompt, max_length=50, temperature=0.7, top_k=30, top_p=0.9) -> str:\n",
    "    '''\n",
    "    Genera un chiste breve basado en el prompt proporcionado.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): El texto inicial para el chiste.\n",
    "        max_length (int): Longitud máxima del chiste generado.\n",
    "        temperature (float): Controla la aleatoriedad.\n",
    "        top_k (int): Tokens más probables considerados en cada paso.\n",
    "        top_p (float): Probabilidad acumulativa para top-p sampling.\n",
    "\n",
    "    Returns:\n",
    "        str: El chiste generado.\n",
    "    '''\n",
    "    # Tokenizar el prompt\n",
    "    input_ids = tokenizer4(prompt, return_tensors='pt').input_ids\n",
    "\n",
    "    # Generar el chiste con parámetros optimizados\n",
    "    output = model4.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        num_return_sequences=1,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer4.eos_token_id,\n",
    "        eos_token_id=tokenizer4.convert_tokens_to_ids(\n",
    "            '<END>')  # Detener al encontrar <END>\n",
    "    )\n",
    "\n",
    "    # Decodificar el chiste generado\n",
    "    joke = tokenizer4.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    return joke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chiste generado:\n",
      "<START>[QUESTION] ¿Que le dijo un judio al otro?\n",
      "[ANSWER] Que lo que me dijo el otro fue que le había dicho que se lo hiciera a otro.\n",
      "<END\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de uso\n",
    "prompt = '<START>[QUESTION] ¿Que le dijo un judio al otro?'\n",
    "chiste = generate_short_joke(prompt)\n",
    "\n",
    "print(f'Chiste generado:\\n{chiste}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
