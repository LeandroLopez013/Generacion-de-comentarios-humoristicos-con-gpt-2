{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'text', 'keywords', 'funny', 'category'],\n",
       "    num_rows: 2419\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"mrm8488/CHISTES_spanish_jokes\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>keywords</th>\n",
       "      <th>funny</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>- ¡Rápido, necesitamos sangre!\\n- Yo soy 0 pos...</td>\n",
       "      <td>sangre</td>\n",
       "      <td>1</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>- ¿Cuál es el mejor portero del mundial? \\n- E...</td>\n",
       "      <td>futbol,porteros</td>\n",
       "      <td>1</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>El otro día unas chicas llamarón a mi puerta y...</td>\n",
       "      <td>dinero,agua</td>\n",
       "      <td>1</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>- Andresito, ¿qué planeta va después de Marte?...</td>\n",
       "      <td>planetas</td>\n",
       "      <td>1</td>\n",
       "      <td>profesiones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>- ¿Por qué Bob Esponja no va al gimnasio? \\n- ...</td>\n",
       "      <td>esponja,gimnasios</td>\n",
       "      <td>1</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Van dos ciegos y le dice uno al otro: \\n- Ojal...</td>\n",
       "      <td>ciegos</td>\n",
       "      <td>1</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Noticia de última hora!! \\n\\nMuere una suegra ...</td>\n",
       "      <td>canarias,coches,noticias</td>\n",
       "      <td>2</td>\n",
       "      <td>familia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>– Mamá, mamá, en el colegio dicen que estoy lo...</td>\n",
       "      <td>locos,sillas</td>\n",
       "      <td>1</td>\n",
       "      <td>familia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>– Mamá, mamá, ¿me haces un bocata de jamón?\\n–...</td>\n",
       "      <td>madres,jamón</td>\n",
       "      <td>1</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>- Qué pasa si te expulsan de cuatro univerdade...</td>\n",
       "      <td>universitarios,universidades</td>\n",
       "      <td>1</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  \\\n",
       "0   0  - ¡Rápido, necesitamos sangre!\\n- Yo soy 0 pos...   \n",
       "1   1  - ¿Cuál es el mejor portero del mundial? \\n- E...   \n",
       "2   2  El otro día unas chicas llamarón a mi puerta y...   \n",
       "3   3  - Andresito, ¿qué planeta va después de Marte?...   \n",
       "4   4  - ¿Por qué Bob Esponja no va al gimnasio? \\n- ...   \n",
       "5   5  Van dos ciegos y le dice uno al otro: \\n- Ojal...   \n",
       "6   6  Noticia de última hora!! \\n\\nMuere una suegra ...   \n",
       "7   7  – Mamá, mamá, en el colegio dicen que estoy lo...   \n",
       "8   8  – Mamá, mamá, ¿me haces un bocata de jamón?\\n–...   \n",
       "9   9  - Qué pasa si te expulsan de cuatro univerdade...   \n",
       "\n",
       "                       keywords  funny     category  \n",
       "0                        sangre      1        otros  \n",
       "1               futbol,porteros      1        otros  \n",
       "2                   dinero,agua      1        otros  \n",
       "3                      planetas      1  profesiones  \n",
       "4             esponja,gimnasios      1        otros  \n",
       "5                        ciegos      1        otros  \n",
       "6      canarias,coches,noticias      2      familia  \n",
       "7                  locos,sillas      1      familia  \n",
       "8                  madres,jamón      1        otros  \n",
       "9  universitarios,universidades      1        otros  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.set_format('pandas')\n",
    "df = dataset.to_pandas()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando el conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import keras\n",
    "import accelerate\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch.nn as nn\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
    "from typing import Optional, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/.pyenv/versions/3.9.15/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"DeepESP/gpt2-spanish\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(max_len):\n",
    "    def _preprocess_function(examples):\n",
    "        return tokenizer(\n",
    "            examples['text'],\n",
    "            max_length=max_len,\n",
    "            truncation=True,\n",
    "            padding='longest',  # Usa el padding más corto posible para cada lote\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "    return _preprocess_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2419/2419 [00:00<00:00, 4317.48 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 1693\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 726\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.reset_format()\n",
    "tokenized_dataset = dataset.map(preprocess_function(max_len=256), batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([col for col in tokenized_dataset.column_names if col != 'input_ids'])\n",
    "tokenized_dataset = tokenized_dataset.train_test_split(train_size=0.70)\n",
    "tokenized_dataset.set_format('torch')\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([ 6907, 28370,   788,   289,   304,  1065,  8981,   268, 36175,  1925,\n",
      "           21,   668,  5720,   297,  1742,   305,   725,   268,   370,  2148,\n",
      "           35,   521,    23,   208,   703, 22266,  1408,   288,  1554,   268,\n",
      "          304,  1079,   299,   788,   289,   297, 29522,   268,   304,  3050,\n",
      "          324,   299,  7792,   307, 11900,   344,  6403,    23,   208, 34089,\n",
      "          432,  1925,  6001,   281,  1593, 28370,   324,   426,  4013,    35,\n",
      "          208,    22,   576, 13376,   299,  1481,   281, 11900,    40,   208,\n",
      "           22,  4340,  2207,  1368,  2324,  1593, 28370,    23,   208,    22,\n",
      "         7520,   913,  2953,   299,   366,  1368, 13365, 36175,  1925,    23,\n",
      "          208,  6907, 28370, 18116,   304, 18597,   268,  4762, 47516,   596,\n",
      "          288,  8460,   324,  2324,    35,   208,    22,   576,    42,   727,\n",
      "        11979,    40,   208, 31361,   289,   297,  1095,   289,   299, 36175,\n",
      "         1925,  6493,   325, 18597,   596,   288,  8460,    21,   297,  1079,\n",
      "        29160,   324,   314, 12476,    23,   208, 34089,   432,  1925,    21,\n",
      "          741, 17097,    21,   426,  1167,   325, 18597,   281,  1593, 28370,\n",
      "          324,   426,  2324,    35,   208,    22,  1674, 22182,   328,   363,\n",
      "        22182,    23, 16550,   785,  2635,    23,   208, 11403,    21,  1593,\n",
      "        28370, 25317,    35,   208,    22,  1602,   488,  2047,   297,  1742,\n",
      "          305,   725,   268,   370,  1039,    35,   521,   324,  2005,   299,\n",
      "          314,  3737,  1030,    23,   208, 34089,   432,  1925, 13365,    35,\n",
      "          208,    22,  1602,   638,   396,   488,  2047,    21,   533,  3855,\n",
      "         7816,   299,   314, 10523,   281, 16261,  1200, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Rajoy está en un bar acompañado de Zapatero, cuando comienza el telediario de las 21:00.\\nEl presentador cuenta la historia de un hombre que está en el ático de un edificio y que amenaza con saltar al vacío.\\nZapatero mira a Rajoy y le pregunta:\\n- ¿Crees que va a saltar?\\n- Eso parece - dice Rajoy.\\n- Pues yo creo que no - responde Zapatero.\\nRajoy coloca un billete de 500 EUR sobre la barra y dice:\\n- ¿Apostamos?\\nJusto en el momento en que Zapatero pone su billete sobre la barra, el hombre salta y se mata.\\nZapatero, muy afectado, le da su billete a Rajoy y le dice:\\n- Una apuesta es una apuesta. Toma tu dinero.\\nLuego, Rajoy admite:\\n- Yo había visto el telediario de las 15:00 y sabía que se tiraría.\\nZapatero responde:\\n- Yo también lo había visto, pero jamás pensé que se volvería a tirar...<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Muestra la primera entrada del dataset tokenizado\n",
    "print(tokenized_dataset['train'][0])\n",
    "\n",
    "# lo decodeamos\n",
    "tokenizer.decode(tokenized_dataset['train'][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/.pyenv/versions/3.9.15/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "epochs = 15\n",
    "batch_size = 8\n",
    "logging_steps = len(tokenized_dataset['train']) // batch_size\n",
    "\n",
    "# Definimos los parámetros globales de entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./hf-gpt',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=epochs,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    disable_tqdm=False,\n",
    "    logging_steps=logging_steps\n",
    ")\n",
    "\n",
    "# Y definimos el entrenador, especificando el modelo, datasets y el tokenizador\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['test'],\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 272/546 [1:30:48<1:28:10, 19.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.4611, 'grad_norm': 4.214391708374023, 'learning_rate': 1.0036630036630037e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 50%|█████     | 273/546 [1:33:45<1:05:19, 14.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.2365965843200684, 'eval_runtime': 173.6238, 'eval_samples_per_second': 1.394, 'eval_steps_per_second': 0.179, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 544/546 [2:59:47<00:37, 18.99s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1615, 'grad_norm': 3.0497474670410156, 'learning_rate': 7.326007326007327e-08, 'epoch': 1.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 546/546 [3:03:06<00:00, 14.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.208950996398926, 'eval_runtime': 175.975, 'eval_samples_per_second': 1.375, 'eval_steps_per_second': 0.176, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n",
      "100%|██████████| 546/546 [3:03:08<00:00, 20.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 10988.4916, 'train_samples_per_second': 0.396, 'train_steps_per_second': 0.05, 'train_loss': 3.309132829253927, 'epoch': 2.0}\n",
      "CPU times: user 16h 17min 18s, sys: 1h 46min 28s, total: 18h 3min 47s\n",
      "Wall time: 3h 3min 8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=546, training_loss=3.309132829253927, metrics={'train_runtime': 10988.4916, 'train_samples_per_second': 0.396, 'train_steps_per_second': 0.05, 'total_flos': 1137665507328000.0, 'train_loss': 3.309132829253927, 'epoch': 2.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el modelo\n",
    "model.save_pretrained('hf-gpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de la entrada: torch.Size([1, 7])\n",
      "Dimensiones de la salida: torch.Size([1, 7, 50257])\n",
      "Dimensiones del último token de la secuencia: torch.Size([50257])\n",
      "Dimensiones de la probabilidad de los tokens: torch.Size([50257])\n",
      "{' no': '10.73%', ' se': '9.15%', ' había': '3.97%', ' me': '2.61%', ' estaba': '2.59%', ',': '2.32%', ' era': '1.64%', ' tenía': '1.48%', ' ha': '1.42%', ' le': '1.24%'}\n"
     ]
    }
   ],
   "source": [
    "text = 'Habia una vez un hombre que'\n",
    "best = 10\n",
    "\n",
    "with torch.no_grad():\n",
    "    tokens = tokenizer(text, return_tensors='pt')['input_ids'].to(device)\n",
    "    print(\"Dimensiones de la entrada:\", tokens.shape)\n",
    "    output = model(input_ids=tokens)\n",
    "    print(\"Dimensiones de la salida:\", output.logits.shape)\n",
    "    output = output.logits[0, -1, :]\n",
    "    print(\"Dimensiones del último token de la secuencia:\", output.shape)\n",
    "    probs = torch.softmax(output, dim=-1)\n",
    "    print(\"Dimensiones de la probabilidad de los tokens:\", probs.shape)\n",
    "    sorted_probs = torch.argsort(probs, dim=-1, descending=True)\n",
    "    print({tokenizer.decode(token): f\"{prob.cpu().numpy() * 100:.2f}%\" for token, prob in zip(sorted_probs[:best], probs[sorted_probs[:best]])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Habia una vez un hombre que no se había bañado ni una sola vez con él.\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple, Optional\n",
    "\n",
    "\n",
    "def generate(\n",
    "    model: nn.Module,  # El modelo de lenguaje neuronal utilizado para generar el texto\n",
    "    # El tokenizador que convierte el texto en tokens para el modelo\n",
    "    tokenizer: PreTrainedTokenizerBase,\n",
    "    start: str,  # Texto de inicio que da el punto de partida para generar el chiste\n",
    "    max_length: int = 50,  # Número máximo de tokens que se generarán\n",
    "    eps: float = 0.5,  # Parámetro para la estrategia e-greedy de selección de tokens\n",
    "    top_n: int = 5,  # Número de tokens con mayor probabilidad a considerar\n",
    "    # Indica si se deben devolver los detalles de las iteraciones\n",
    "    return_iterations: bool = False,\n",
    "    # El dispositivo donde se ejecutará el modelo (por ejemplo, CPU o GPU)\n",
    "    device: str = \"cpu\",\n",
    "    # Lista de tokens que marcan el final del chiste\n",
    "    stop_tokens: List[str] = ['.', '!', '?']\n",
    ") -> Tuple[str, Optional[pd.DataFrame]]:\n",
    "\n",
    "    # Inicializamos la lista 'output' con el texto inicial proporcionado\n",
    "    output = [start]\n",
    "    iterations = []  # Lista para almacenar los detalles de cada iteración si se solicita\n",
    "    with torch.no_grad():  # Desactiva el cálculo de gradientes para ahorrar memoria, ya que no entrenamos el modelo\n",
    "        # Convierte el último texto generado en tokens y los mueve al dispositivo especificado\n",
    "        input_ids = tokenizer(\n",
    "            output[-1], return_tensors='pt')['input_ids'].to(device)\n",
    "\n",
    "        # Bucle principal para generar nuevos tokens hasta alcanzar la longitud máxima\n",
    "        for _ in range(max_length):\n",
    "            # Obtiene los logits (probabilidades sin normalizar) del modelo\n",
    "            logits = model(input_ids=input_ids).logits\n",
    "            # Aplica softmax para convertir los logits en probabilidades\n",
    "            probs = torch.softmax(logits[0, -1, :], dim=-1)\n",
    "            # Ordena los tokens por probabilidad, en orden descendente\n",
    "            sorted_tokens = torch.argsort(probs, dim=-1, descending=True)\n",
    "\n",
    "            # Estrategia e-greedy para seleccionar el siguiente token:\n",
    "            # Si el valor aleatorio es menor que 'eps', se elige el token más probable,\n",
    "            # de lo contrario, se selecciona uno basado en la distribución de probabilidades.\n",
    "            if np.random.random_sample(1)[0] < eps:\n",
    "                next_token = sorted_tokens[0].unsqueeze(\n",
    "                    dim=0)  # Selecciona el token más probable\n",
    "            else:\n",
    "                # Selecciona un token basado en la probabilidad\n",
    "                next_token = torch.multinomial(probs, 1)\n",
    "\n",
    "            # Convierte el token seleccionado a texto\n",
    "            next_word = tokenizer.decode(next_token)\n",
    "\n",
    "            # Verifica si el token generado es un signo de puntuación que marca el final del chiste\n",
    "            if any(stop in next_word for stop in stop_tokens):\n",
    "                output.append(next_word)  # Añade el token final al output\n",
    "                break  # Detiene la generación, ya que se alcanzó el remate del chiste\n",
    "\n",
    "            # Si 'return_iterations' es True, almacenamos detalles de la iteración actual para análisis\n",
    "            if return_iterations:\n",
    "                # El texto generado hasta el momento\n",
    "                iteration = {'input': ''.join(output)}\n",
    "                # Obtiene los 'top_n' tokens más probables y sus respectivas probabilidades\n",
    "                best_n = sorted_tokens[:top_n].cpu().tolist()\n",
    "                # Guarda la información de cada token en el DataFrame\n",
    "                choices = {f'Choice #{choice+1}': f'{tokenizer.decode(token)} ({prob:.4f})'\n",
    "                           for choice, (token, prob) in enumerate(zip(best_n, probs[best_n].cpu().tolist()))}\n",
    "                iteration.update(choices)\n",
    "                iterations.append(iteration)\n",
    "\n",
    "            # Añade la palabra generada al 'output'\n",
    "            output.append(next_word)\n",
    "            # Actualiza los 'input_ids' concatenando el nuevo token generado\n",
    "            input_ids = torch.cat(\n",
    "                [input_ids, next_token.unsqueeze(dim=0)], dim=-1)\n",
    "\n",
    "        # Convierte la lista de tokens generados en una cadena de texto\n",
    "        output_text = ''.join(output)\n",
    "\n",
    "        # Devuelve el texto generado y, opcionalmente, las iteraciones si se solicitó\n",
    "        if not return_iterations:\n",
    "            return output_text, None\n",
    "        else:\n",
    "            # Convierte las iteraciones en un DataFrame\n",
    "            df = pd.DataFrame(iterations)\n",
    "            return output_text, df\n",
    "\n",
    "\n",
    "# Ejemplo de uso\n",
    "output_text, _ = generate(model, tokenizer, text,\n",
    "                          max_length=50, eps=0.2, device=device)\n",
    "print(output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
