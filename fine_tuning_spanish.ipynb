{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/MEGA/text_mining/text-mining/env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-10-04 15:09:58.969389: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-04 15:09:58.988381: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-04 15:09:58.993181: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-04 15:09:59.008538: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-04 15:10:00.161547: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import (Trainer, TrainingArguments, EarlyStoppingCallback,\n",
    "                          DataCollatorForLanguageModeling, AutoModelForCausalLM, AutoTokenizer)\n",
    "from datasets import load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'text', 'keywords', 'funny', 'category'],\n",
       "    num_rows: 2419\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('mrm8488/CHISTES_spanish_jokes', split='train')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>keywords</th>\n",
       "      <th>funny</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>- ¡Rápido, necesitamos sangre!\\n- Yo soy 0 pos...</td>\n",
       "      <td>sangre</td>\n",
       "      <td>1</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>- ¿Cuál es el mejor portero del mundial? \\n- E...</td>\n",
       "      <td>futbol,porteros</td>\n",
       "      <td>1</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>El otro día unas chicas llamarón a mi puerta y...</td>\n",
       "      <td>dinero,agua</td>\n",
       "      <td>1</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>- Andresito, ¿qué planeta va después de Marte?...</td>\n",
       "      <td>planetas</td>\n",
       "      <td>1</td>\n",
       "      <td>profesiones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>- ¿Por qué Bob Esponja no va al gimnasio? \\n- ...</td>\n",
       "      <td>esponja,gimnasios</td>\n",
       "      <td>1</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Van dos ciegos y le dice uno al otro: \\n- Ojal...</td>\n",
       "      <td>ciegos</td>\n",
       "      <td>1</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Noticia de última hora!! \\n\\nMuere una suegra ...</td>\n",
       "      <td>canarias,coches,noticias</td>\n",
       "      <td>2</td>\n",
       "      <td>familia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>– Mamá, mamá, en el colegio dicen que estoy lo...</td>\n",
       "      <td>locos,sillas</td>\n",
       "      <td>1</td>\n",
       "      <td>familia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>– Mamá, mamá, ¿me haces un bocata de jamón?\\n–...</td>\n",
       "      <td>madres,jamón</td>\n",
       "      <td>1</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>- Qué pasa si te expulsan de cuatro univerdade...</td>\n",
       "      <td>universitarios,universidades</td>\n",
       "      <td>1</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  \\\n",
       "0   0  - ¡Rápido, necesitamos sangre!\\n- Yo soy 0 pos...   \n",
       "1   1  - ¿Cuál es el mejor portero del mundial? \\n- E...   \n",
       "2   2  El otro día unas chicas llamarón a mi puerta y...   \n",
       "3   3  - Andresito, ¿qué planeta va después de Marte?...   \n",
       "4   4  - ¿Por qué Bob Esponja no va al gimnasio? \\n- ...   \n",
       "5   5  Van dos ciegos y le dice uno al otro: \\n- Ojal...   \n",
       "6   6  Noticia de última hora!! \\n\\nMuere una suegra ...   \n",
       "7   7  – Mamá, mamá, en el colegio dicen que estoy lo...   \n",
       "8   8  – Mamá, mamá, ¿me haces un bocata de jamón?\\n–...   \n",
       "9   9  - Qué pasa si te expulsan de cuatro univerdade...   \n",
       "\n",
       "                       keywords  funny     category  \n",
       "0                        sangre      1        otros  \n",
       "1               futbol,porteros      1        otros  \n",
       "2                   dinero,agua      1        otros  \n",
       "3                      planetas      1  profesiones  \n",
       "4             esponja,gimnasios      1        otros  \n",
       "5                        ciegos      1        otros  \n",
       "6      canarias,coches,noticias      2      familia  \n",
       "7                  locos,sillas      1      familia  \n",
       "8                  madres,jamón      1        otros  \n",
       "9  universitarios,universidades      1        otros  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.set_format('pandas')\n",
    "df = dataset.to_pandas()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Andresito, ¿qué planeta va después de Marte? \n",
      "- Miércole, señorita. <fin>\n"
     ]
    }
   ],
   "source": [
    "def jokes_last(df):\n",
    "    '''\n",
    "    Agrega <fin> al final de cada chiste si no está presente\n",
    "    '''\n",
    "    df['text'] = df['text'].apply(\n",
    "        lambda x: x.strip() if x.strip().endswith('<fin>') else x.strip() + ' <fin>')\n",
    "    return df\n",
    "\n",
    "\n",
    "df = jokes_last(df)\n",
    "print(df['text'][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando el conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/MEGA/text_mining/text-mining/env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"DeepESP/gpt2-spanish\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(max_len):\n",
    "    def _preprocess_function(examples):\n",
    "        return tokenizer(\n",
    "            examples['text'],\n",
    "            max_length=max_len,\n",
    "            truncation=True,\n",
    "            padding='longest',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "    return _preprocess_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 1693\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 726\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.reset_format()\n",
    "tokenized_dataset = dataset.map(preprocess_function(max_len=256), batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([col for col in tokenized_dataset.column_names if col != 'input_ids'])\n",
    "tokenized_dataset = tokenized_dataset.train_test_split(train_size=0.70)\n",
    "tokenized_dataset.set_format('torch')\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([ 2243, 16195,    21,   268,  2464,   304,  1076, 14752,    21,   314,\n",
      "         3854,   281,   304, 36008,   293,   324,   426,  2324,    35,   208,\n",
      "           22,   576,  2776,  4168,   297,  1465,   297, 37595,   296,    40,\n",
      "          208,    22,  7520,  4199,   307,  2129,  9113,    21, 15558,  3837,\n",
      "          337, 16064,   313,    23, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Un camarero, de esos un poco pelota, se acerca a un comensal y le dice:\\n- ¿Cómo encontró el señor el filete?\\n- Pues realmente con mucha dificultad, escondido debajo del puré.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Muestra la primera entrada del dataset tokenizado\n",
    "print(tokenized_dataset['train'][0])\n",
    "\n",
    "# lo decodeamos\n",
    "tokenizer.decode(tokenized_dataset['train'][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 8\n",
    "logging_steps = len(tokenized_dataset['train']) // batch_size\n",
    "\n",
    "# Definimos los parámetros globales de entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./hf-gpt',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=epochs,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy='epoch',  # Cambiado de evaluation_strategy a eval_strategy\n",
    "    save_strategy='epoch',  # Guardamos al final de cada época\n",
    "    load_best_model_at_end=True,  # Cargamos el mejor modelo al final\n",
    "    disable_tqdm=False,\n",
    "    logging_steps=logging_steps,\n",
    "    save_total_limit=2,\n",
    "    # Utilizamos eval_loss como métrica de referencia\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False  # Indica que buscamos minimizar eval_loss\n",
    ")\n",
    "\n",
    "# Crear el callback para early stopping, asegurándote de que no se repita\n",
    "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=3)\n",
    "\n",
    "# Definir el entrenador, asegurando que no se añadan múltiples callbacks\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False),\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[early_stopping_callback]  # Solo se añade una vez\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descripcion de los argumentos:\n",
    "- `num_train_epochs`: Número de iteraciones de entrenamiento.\n",
    "- `learning_rate`: Tasa de aprendizaje para el optimizador, Un valor más alto puede acelerar el entrenamiento, pero un valor demasiado alto puede hacer que el modelo no converja.\n",
    "- `per_device_train_batch_size`: Tamaño del lote por dispositivo de entrenamiento.\n",
    "- `per_device_eval_batch_size`: Tamaño del lote por dispositivo de evaluación.\n",
    "- `weight_decay`: Tasa de decaimiento de los pesos, que ayuda a evitar el sobreajuste al añadir una penalización a los pesos grandes.\n",
    "- `eval_strategy`: significa que evalua al final de cada epoch.\n",
    "- `save_strategy`: significa que guarda el modelo al final de cada epoch.\n",
    "- `load_best_model_at_end`: significa que carga el mejor modelo al final del entrenamiento.\n",
    "- `logging_steps`: Cada cuántos pasos se imprime el log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='848' max='2120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 848/2120 1:43:22 < 2:35:25, 0.14 it/s, Epoch 4/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.481500</td>\n",
       "      <td>3.176255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.161100</td>\n",
       "      <td>3.216372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.114400</td>\n",
       "      <td>3.227968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.919600</td>\n",
       "      <td>3.261066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=848, training_loss=2.168072089834033, metrics={'train_runtime': 6211.2886, 'train_samples_per_second': 2.726, 'train_steps_per_second': 0.341, 'total_flos': 3144580551671808.0, 'train_loss': 2.168072089834033, 'epoch': 4.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('trained-gpt2-with-stop/tokenizer_config.json',\n",
       " 'trained-gpt2-with-stop/special_tokens_map.json',\n",
       " 'trained-gpt2-with-stop/vocab.json',\n",
       " 'trained-gpt2-with-stop/merges.txt',\n",
       " 'trained-gpt2-with-stop/added_tokens.json',\n",
       " 'trained-gpt2-with-stop/tokenizer.json')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Guardamos el modelo (solo correr si se vuelve a entrenar el modelo)\n",
    "#trainer.save_model('trained-gpt2-with-stop')\n",
    "#tokenizer.save_pretrained('trained-gpt2-with-stop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correr para cargar el modelo pre-entrenado\n",
    "model = AutoModelForCausalLM.from_pretrained('trained-gpt2-with-stop')\n",
    "tokenizer = AutoTokenizer.from_pretrained('trained-gpt2-with-stop')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
